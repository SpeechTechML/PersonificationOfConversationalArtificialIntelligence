# --- RoBERTa fine-tuning config ---

datasets:
  # - "DailyDialog"
  - "MELD"
  # - "IEMOCAP"

task:
  label_encoder:
    save_dir: "checkpoints"

  dataset:
    dialogue_context_size: 5
    sep_token: "</s>"
    batch_size: 128
    aug_factor: 4

  tokenizer:
    pretrained_model_name_or_path: "blinoff/roberta-base-russian-v0"
    # pretrained_model_name_or_path: "j-hartmann/emotion-english-distilroberta-base"
    max_len: 512

  model:
    pretrained_model_name_or_path: "blinoff/roberta-base-russian-v0"
    # pretrained_model_name_or_path: "j-hartmann/emotion-english-distilroberta-base"
    num_labels: 7

  training_args:
    # Common
    num_train_epochs: 20
    learning_rate: 0.000001
    gradient_accumulation_steps: 2

    # Evaluation
    per_device_eval_batch_size: 64
    evaluation_strategy: "steps"
    eval_steps: 1000

    # Saving
    save_strategy: "epoch"
    save_steps: 1
    output_dir: "temp"