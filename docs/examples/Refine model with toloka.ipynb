{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-z4U--8dMtB",
    "outputId": "20c975a9-cad6-4bf1-f2fc-5fdd9fedfd9a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NnGKaPrCcnaU"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH2WFUiev8ox"
   },
   "outputs": [],
   "source": [
    "models_path = \"C:/Users/Stud/Documents/models/\"\n",
    "\n",
    "MODEL_NAMES = {\n",
    "    \"t5-large\": models_path + \"ruT5-large\",\n",
    "    \"mt\": models_path + \"mt\",\n",
    "    \"dialogpt3\": models_path + \"dialogpt3\",\n",
    "    \"t5-base-trained\": models_path + \"t5-base-trained\",\n",
    "    \"mt-trained\": models_path + \"mt-trained\",\n",
    "    }\n",
    "\n",
    "you_token = '<you>'\n",
    "other_token = '<oth>'\n",
    "persona_token = '<per>'\n",
    "ATTR_TO_SPECIAL_TOKEN = {'additional_special_tokens': [you_token, other_token, persona_token]}\n",
    "\n",
    "data_path = \"C:/Users/Stud/Documents/datasets/toloka_speller.txt\"\n",
    "aug_path = \"/content/drive/MyDrive/test_both_original_aug_ru2.json\"\n",
    "\n",
    "save_path = models_path + \"rudialogpt3-trained\"\n",
    "\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qnlrANivfOPJ"
   },
   "outputs": [],
   "source": [
    "path = MODEL_NAMES['dialogpt3']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(path, max_length=max_length, output_attentions=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, max_length=max_length, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
    "\n",
    "model.resize_token_embeddings(new_num_tokens=model.config.vocab_size + num_added_tokens)\n",
    "\n",
    "#Only for MT model\n",
    "#model.target_vocab_size=model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dq4HKx3_TVgL",
    "outputId": "696baf95-ab92-4b6c-a2e0-f2297df73c74"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}, {param.dtype}\\|  grad-{param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQpgTep0XJoA",
    "outputId": "80573f99-90ce-46d8-f68d-eb9acba4c984"
   },
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkJ5qZpiHP0o"
   },
   "outputs": [],
   "source": [
    "#Augmented data\n",
    "\n",
    "with open(aug_path, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "aug_context = []\n",
    "aug_labels = []\n",
    "original_persona1 = None\n",
    "original_persona2 = None\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "\n",
    "    if original_persona1 is None:\n",
    "        original_persona1 = data['persona']\n",
    "    elif original_persona2 is None:\n",
    "        original_persona2 = data['persona']\n",
    "    elif original_persona1 != data['persona'] and original_persona2 != data['persona']:\n",
    "        original_persona1 = data['persona']\n",
    "        original_persona2 = None\n",
    "\n",
    "    dialogue = data['context']\n",
    "\n",
    "    other_persona = None\n",
    "    your_persona = []\n",
    "    if original_persona1 == data['persona']:\n",
    "        d_len = max_length*2 - len(original_persona1)\n",
    "        other_persona = original_persona2\n",
    "    else:\n",
    "        d_len = max_length*2 - len(original_persona2)\n",
    "        other_persona = original_persona1\n",
    "    for persona in data['persona_aug']:\n",
    "        your_persona.append(persona[random.randint(0, len(persona)-1)])\n",
    "\n",
    "    your_persona = (persona_token + persona_token.join(your_persona)).replace('.', '')\n",
    "\n",
    "    label = you_token + data['responce_aug'][random.randint(0, len(data['responce_aug'])-1)]\n",
    "\n",
    "    dialogue_history = \"\"\n",
    "\n",
    "    start_token = you_token if len(data['context']) % 2 == 0 else other_token\n",
    "    for j in range(len(dialogue)-1, 0, -1):\n",
    "        if len(dialogue[j] + dialogue_history) <= d_len:\n",
    "            dialogue_history = start_token + dialogue[j] + dialogue_history\n",
    "            start_token = you_token if start_token != you_token else other_token\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if dialogue_history != \"\":\n",
    "        aug_context.append(your_persona + dialogue_history)\n",
    "        aug_labels.append(label)\n",
    "\n",
    "aug_dataset = {\n",
    "    \"context\": aug_context,\n",
    "    \"labels\": aug_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z-E-ddUUc-UI"
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "context = []\n",
    "\n",
    "with open(data_path, 'r', encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "\n",
    "    for i in range(0, len(lines), 3):\n",
    "        persona1 = lines[i].replace('\\n', '').replace('|', persona_token).replace('.', '')\n",
    "        persona2 = lines[i+1].replace('\\n', '').replace('|', persona_token).replace('.', '')\n",
    "\n",
    "        dialogue = lines[i+2].replace(\"Пользователь 1: \", \"\\n<p1>\").replace(\"Пользователь 2: \", \"\\n<p2>\").split(\"\\n\")\n",
    "        dialogue.pop(0)\n",
    "\n",
    "        for i in range(1, len(dialogue) - 1):\n",
    "            your_persona = \"\"\n",
    "            persona_id = \"\"\n",
    "            if dialogue[i+1][:4] == \"<p1>\":\n",
    "                d_len = max_length*2 - len(persona1)\n",
    "                your_persona = persona1\n",
    "                persona_id = \"<p1>\"\n",
    "            else:\n",
    "                d_len = max_length*2 - len(persona2)\n",
    "                your_persona = persona2\n",
    "                persona_id = \"<p2>\"\n",
    "            \n",
    "            label = you_token + dialogue[i + 1][4:]\n",
    "            dialogue_history = \"\"\n",
    "\n",
    "            for j in range(i, 0, -1):\n",
    "                if len(dialogue[j][4:] + dialogue_history) <= d_len:\n",
    "                    if dialogue[j][:4] == persona_id:\n",
    "                        dialogue_history = you_token + dialogue[j][4:] + dialogue_history\n",
    "                    else:\n",
    "                        dialogue_history = other_token + dialogue[j][4:] + dialogue_history\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if dialogue_history != \"\":\n",
    "                #'<s>' only for DialoGPT3\n",
    "                context.append('<s>' + your_persona + dialogue_history)\n",
    "                labels.append(dialogue_history)\n",
    "\n",
    "dataset = {\n",
    "    \"context\": context, \n",
    "    \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wALqJXWMFP_Q",
    "outputId": "7a9453ed-dfb7-4ac5-e47c-a01190d4c676"
   },
   "outputs": [],
   "source": [
    "dataset[\"context\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"<s><per>Я фитнестренер<per>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0, 4, 50257, 50258, 50259]\n",
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<you>', '<oth>', '<per>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xGpBOFhXKTZq"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    texts, labels = zip(*data)\n",
    "    \n",
    "    other_token = 50258\n",
    "    you_token = 50257\n",
    "    bos_token = 1\n",
    "    eos_token = 2\n",
    "    \n",
    "    labels = tokenizer(list(labels), max_length=max_length, truncation=True)[\"input_ids\"]\n",
    "\n",
    "    inputs = tokenizer(list(texts), return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        temp = []\n",
    "        j = 1\n",
    "        while inputs['input_ids'][i][j+1] != other_token and inputs['input_ids'][i][j+1] != you_token:\n",
    "            temp += [-100]\n",
    "            j += 1\n",
    "        labels[i] = temp + [bos_token] + labels[i]\n",
    "        labels[i].extend([eos_token])\n",
    "        labels[i].extend([-100 for _ in range(len(labels[i]), max_length)])\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    if 'token_type_ids' in inputs:\n",
    "        inputs.pop('token_type_ids')\n",
    "\n",
    "    inputs['labels'] = labels\n",
    "\n",
    "    return inputs\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.texts = list(data['context'])\n",
    "        self.labels = list(data['labels'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if hasattr(self, 'labels'):\n",
    "            return self.texts[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.texts[idx], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9H5CDfMPL4W5",
    "outputId": "ba9c589b-1e55-44f5-803f-1eebd1ec6e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189236\n",
      "47309\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset['labels'])\n",
    "\n",
    "train_size = int(0.8*dataset_size)\n",
    "eval_size = dataset_size - train_size\n",
    "print(train_size)\n",
    "print(eval_size)\n",
    "data = TextDataset(dataset)\n",
    "train, val = torch.utils.data.random_split(\n",
    "    data, \n",
    "    [train_size, eval_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gDtEsa-0N8zP"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TZonwbeQUZcC"
   },
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "\n",
    "def compute_bleu_f1(model, inputs, labels):\n",
    "    #metric = load_metric(\"bleu\")\n",
    "    results = {'bleu': [],\n",
    "               'f1': [],\n",
    "               'recall': []}\n",
    "\n",
    "    preds = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        no_repeat_ngram_size=3\n",
    "    ).to('cpu').detach().numpy()\n",
    "    \n",
    "    preds = list(preds)\n",
    "    str_preds = []\n",
    "    str_labels = []\n",
    "    for i in range(len(preds)):\n",
    "        line = []\n",
    "        for j in range(len(preds[i])):\n",
    "            line.append(str(preds[i][j]))\n",
    "        str_preds.append(line)\n",
    "            \n",
    "    for i in range(len(labels)):\n",
    "        line = []\n",
    "        for j in range(len(labels[i])):\n",
    "            line.append(str(labels[i][j]))\n",
    "        str_labels.append(line)\n",
    "        while '-100' in str_labels[i]:\n",
    "            str_labels[i].remove('-100')\n",
    "\n",
    "    for pred, label in zip(str_preds, str_labels):\n",
    "        #print(f\"{len(label)}, {len(pred)}\")\n",
    "        if len(label) < len(pred):\n",
    "            label.extend(['-100' for _ in range(len(label), len(pred))])\n",
    "        elif len(label) > len(pred):\n",
    "            pred.extend([str(tokenizer.pad_token_id) for _ in range(len(pred), len(label))])\n",
    "\n",
    "        results['bleu'].append(bleu_score([pred], [[label]], max_n=4, weights=[0.25, 0.25, 0.25, 0.25]))\n",
    "        results['f1'].append(f1_score(label, pred, average='macro'))\n",
    "        results['recall'].append(recall_score(label, pred, average='macro'))\n",
    "    return results\n",
    "    \n",
    "\n",
    "def eval_model(model, val_loader, skip_generation=False):\n",
    "    print(\"Validation\")\n",
    "    losses = []\n",
    "    bleu = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "\n",
    "    #if not skip_generation:\n",
    "    #    _, val = torch.utils.data.random_split(data, [dataset_size - 320, 320])\n",
    "    #    val_loader = torch.utils.data.DataLoader(val, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    progress_bar = tqdm(range(len(val_loader)))\n",
    "\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits.to('cpu')\n",
    "\n",
    "        labels = batch.pop('labels').to('cpu').detach().numpy()\n",
    "\n",
    "        if not skip_generation:\n",
    "            results = compute_bleu_f1(model, batch, labels)\n",
    "            bleu.extend(results['bleu'])\n",
    "            f1.extend(results['f1'])\n",
    "            recall.extend(results['recall'])\n",
    "\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.to('cpu').detach().numpy())\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"val loss: {np.mean(losses)}\")\n",
    "    print(f\"val ppl: {np.exp(np.mean(losses))}\")\n",
    "    if not skip_generation:\n",
    "        print(f\"val bleu: {np.mean(bleu)}\")\n",
    "        print(f\"val f1: {np.mean(f1)}\")\n",
    "        print(f\"val recall: {np.mean(recall)}\")\n",
    "\n",
    "    #if np.exp(np.mean(losses)) < 80:\n",
    "    #    model.save_pretrained(\"/content/drive/MyDrive/BB_90_BPE/\")\n",
    "    #    tokenizer.save_pretrained(\"/content/drive/MyDrive/BB_90_BPE/\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader,\n",
    "                num_epochs, optimizer, scheduler,\n",
    "                accumulation_steps, fp16):\n",
    "    ppl = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        print(\"Training\")\n",
    "        progress_bar = tqdm(range(len(train_loader)))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        i = 1\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if i % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "            i += 1\n",
    "        \n",
    "        eval_model(model, val_loader, skip_generation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aef9d9e4615841c1a39cf6c6691f7d1d",
      "92394a999ba648489187f8640a3103f7",
      "90211e78352f4c8d937977ff69356f9a",
      "09f0b29108a548368f5938db42157959",
      "12001d20aae1463497f64dcccf5ec326",
      "80f8797be6114b2ca926328de770a154",
      "897fcb0d776c488a921a20dafa493646",
      "2bae1963aee74b758b14fa80d6c73bd8",
      "ad578853ba7a436cb60d7afbfac33fe2",
      "c745a589d5e5447d88b88d189fc4589a",
      "e50df2a91f5a489ea4cfa6c8e0e9771f"
     ]
    },
    "id": "g4Z45Cwnz62U",
    "outputId": "8345b164-187b-42a5-b830-ba2b55fd2cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aace3b79fac24b5fb798c4a882e3bbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f42dfce1e241a5a2bda48a411e1b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 4.322206974029541\n",
      "val ppl: 75.35475158691406\n",
      "Epoch: 2\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9730823354a28a403d7c9f7bfda25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab97cef5de45feac042fdeca509a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 3.9818365573883057\n",
      "val ppl: 53.61540985107422\n",
      "Epoch: 3\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f3fdbe66324de9bb85ed2c4bc0c0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c5c795c733410bb50e7a5694f6f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 3.6982533931732178\n",
      "val ppl: 40.3767204284668\n",
      "Epoch: 4\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e975ccec408844c3b048fdf3449c83e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7d40e4c35c4e65a8f2b201c5385698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 3.4333341121673584\n",
      "val ppl: 30.979761123657227\n",
      "Epoch: 5\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aa0ba28c9647c2b9c9bd1b941d4ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12060\\2181475080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m train_model(model, train_loader, val_loader,\n\u001b[0;32m     35\u001b[0m             \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m            )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12060\\2865016434.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, optimizer, scheduler, accumulation_steps, fp16)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12060\\2865016434.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "def config_training(layers, lr, num_epochs, num_warmup_steps):\n",
    "    if layers != None:\n",
    "        for i, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            for layer in layers:\n",
    "                if layer in i and param.requires_grad == False:\n",
    "                    param.requires_grad = True\n",
    "                    print(f\"{i}:   {param.shape}\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "layers = None\n",
    "num_epochs = 5\n",
    "optimizer, lr_scheduler = config_training(layers, lr=5e-5,\n",
    "                                          num_epochs=num_epochs,\n",
    "                                          num_warmup_steps=500)\n",
    "\n",
    "train_model(model, train_loader, val_loader,\n",
    "            num_epochs, optimizer, lr_scheduler,\n",
    "            accumulation_steps=16, fp16=False\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "68acb723da3c4b2290c85b54bbe701bb",
      "ba36f81d841e4f379d118a25d34a676e",
      "a5bb6be7b50840ddb78e799b12cdae5b",
      "89d2c7b2efb04a4bb05cab16b9f386c0",
      "435aa234062940b2bc2b78951ecbde4c",
      "f0ae577f2dd24c7c8a6a386bc37c7fc8",
      "aafd5fd722704233b9e4f2d14513dcef",
      "57239a592386457683607013e2b5889b",
      "04f9af659c9a4c6a8353de757c860f31",
      "17ab0583183c4c9a894c0f32ac653a03",
      "8ff6f8e7583545009a114779737fc21d"
     ]
    },
    "id": "Yl9d856lOaN3",
    "outputId": "3669029d-03ae-48d0-e281-fb745e61d6bf"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "eval_model(model, val_loader, skip_generation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jdmUnWJ2QdIq",
    "outputId": "6ec80181-93f1-42ff-d264-cf72773c8293"
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "\n",
    "encoder_input_ids = tokenizer(\"А откуда ты приехал к нам?\", return_tensors=\"pt\", add_special_tokens=True).input_ids.to('cuda')\n",
    "decoder_input_ids = tokenizer(\"Я в Краснодаре жил 5 лет, потом вот сюда приехал.\", return_tensors=\"pt\", add_special_tokens=True).input_ids.to('cuda')\n",
    "\n",
    "outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n",
    "decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
    "\n",
    "model_view(\n",
    "    encoder_attention=outputs.encoder_attentions,\n",
    "    decoder_attention=outputs.decoder_attentions,\n",
    "    cross_attention=outputs.cross_attentions,\n",
    "    encoder_tokens= encoder_text,\n",
    "    decoder_tokens = decoder_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "Xq-A9mX5TLzW",
    "outputId": "ee029a28-b824-4756-d1b5-b3a3e1b97126"
   },
   "outputs": [],
   "source": [
    "head_view(\n",
    "    encoder_attention=outputs.encoder_attentions,\n",
    "    decoder_attention=outputs.decoder_attentions,\n",
    "    cross_attention=outputs.cross_attentions,\n",
    "    encoder_tokens= encoder_text,\n",
    "    decoder_tokens = decoder_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQT9dqeHBN0S"
   },
   "source": [
    "# Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHyaPAqFuxtg",
    "outputId": "40a06bda-5df5-4fd7-f5f6-efc428c5b4c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я программист\n",
      "Живу в Ростове\n",
      "Не умею играть на гитаре\n",
      "Обожаю готовить\n",
      "Не люблю кошек\n",
      "________________________________________\n",
      "Ты умеешь играть на гитаре?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Да У есть гита тебя какие?у естьКК?КЕ?МТ вообще работаюстом но в время рыба в хожу собира модели их зовутияимаатимяинурое���ха�к������������\n",
      "Candidate 2: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Нится,)Ну бывает подру)А любишь?)Не не, на играю на,х освоить у есть то умеешь нету на больше умею ты������������(����))��������������������😄��\n",
      "Candidate 3: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Лю играть гита люблю, не гита, мараючок поциюсо)))ты?Седи,юемцыФты?мят,к я естьч в уппеч тыуптаправраканммм������ло\n",
      "Candidate 4: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Я играть гита, я не играть песни музыке слушаюНе гита вотюрик ты?ДауУ да своеобраз голосДа отличною уяс!����������)������9����������������(�()�\n",
      "Candidate 5: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Яю!Что настро?Шрно лут!ШрнобеюА вообщею я программи, и на программи ячусь колледже не пойти работу я не кошек тоже.?нетопыт����������������������������\n",
      "Candidate 6: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?О безумно) я слушаю наанею это мое)Да)даблю немного, спортю меня моими научи не))ха есть?)Нет ты?нил�������))��)�������������������\n",
      "Candidate 7: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Да я программи, Ростова ты нет у пока на, наре вальист у есть?Да исыскогояА?дасы*Даят есть?Чтомедарсыме*м�������сл))��*����\n",
      "Candidate 8: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?ДаПривет меня Иван умею наре умею гита у естьА тебяА меня у есть ко я собакойнимает собакдор родителейэто их�шно���������������������������😂�������!!\n",
      "Candidate 9: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?Я нет у слух.Б не.. умею зато печенье готовить обожа. готовить просто моестиемое я кошек, меняугафйуфаПон)))рас еще�а�������ха�и��������������������������\n",
      "Candidate 10: Я программистЖиву в РостовеНе умею играть на гитареОбожаю готовитьНе люблю кошекТы умеешь играть на гитаре?ОетьКак)Ую нас тобойка ты ииешь анд?ДаЯ, да оченьи люблю игру гита ты какогоногоальяалья�оууль���������������😂�����������������\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "persona = [\"Я программист\",\n",
    "           \"Живу в Ростове\",\n",
    "           \"Не умею играть на гитаре\",\n",
    "           \"Обожаю готовить\",\n",
    "           \"Не люблю кошек\"]\n",
    "\n",
    "personas = [\"<per>Я программист<per>Живу в Ростове<per>Не умею играть на гитаре<per>Работаю два дня<per>Не люблю кошек\",\n",
    "            \"<per>Я домохозяйка<per>Вышла замуж после школы<per>Есть двое детей<per>Обожаю готовить<per>Мечтаю об отпуске\"]\n",
    "\n",
    "utterance = \"Ты умеешь играть на гитаре?\"\n",
    "\n",
    "for i in persona:\n",
    "    print(i)\n",
    "print(\"________________________________________\")\n",
    "print(utterance + '\\n')\n",
    "outs = []\n",
    "\n",
    "context = \"<per>\" + \"<per>\".join(persona) + \"<oth>\" + utterance\n",
    "\n",
    "inputs = tokenizer.encode(\n",
    "    context,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "for i in range(5):\n",
    "    outs.append(model.generate(\n",
    "        inputs,\n",
    "        #min_length=5,\n",
    "        #max_length=25,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_k=100,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=2\n",
    "        ))\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(f\"Candidate {i*2 + j + 1}: \" + tokenizer.decode(outs[i][j], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "BACaFpX1A-DM",
    "outputId": "b817b2b4-7729-4def-d73b-6bb8f870acdd"
   },
   "outputs": [],
   "source": [
    "persona = \"<per>Я программист<per>Люблю кошек<per>Есть двое детей<per>Работаю два дня<per>Ем пиццу\"\n",
    "dialogue_session = persona\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "stop_word = \"stop\"\n",
    "inp = input(\"In: \")\n",
    "while inp != stop_word:\n",
    "    dialogue_session += other_token + inp\n",
    "    inputs = tokenizer.encode(dialogue_session, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        #min_length=5,\n",
    "        #max_length=25,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Out: \" + decoded + \"\\n\")\n",
    "    dialogue_session += you_token + decoded\n",
    "\n",
    "    inp = input(\"In: \")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Toloka.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "someenv",
   "language": "python",
   "name": "someenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04f9af659c9a4c6a8353de757c860f31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09f0b29108a548368f5938db42157959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c745a589d5e5447d88b88d189fc4589a",
      "placeholder": "​",
      "style": "IPY_MODEL_e50df2a91f5a489ea4cfa6c8e0e9771f",
      "value": " 1769/11828 [09:20&lt;53:26,  3.14it/s]"
     }
    },
    "12001d20aae1463497f64dcccf5ec326": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17ab0583183c4c9a894c0f32ac653a03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bae1963aee74b758b14fa80d6c73bd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "435aa234062940b2bc2b78951ecbde4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57239a592386457683607013e2b5889b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68acb723da3c4b2290c85b54bbe701bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba36f81d841e4f379d118a25d34a676e",
       "IPY_MODEL_a5bb6be7b50840ddb78e799b12cdae5b",
       "IPY_MODEL_89d2c7b2efb04a4bb05cab16b9f386c0"
      ],
      "layout": "IPY_MODEL_435aa234062940b2bc2b78951ecbde4c"
     }
    },
    "80f8797be6114b2ca926328de770a154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "897fcb0d776c488a921a20dafa493646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89d2c7b2efb04a4bb05cab16b9f386c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17ab0583183c4c9a894c0f32ac653a03",
      "placeholder": "​",
      "style": "IPY_MODEL_8ff6f8e7583545009a114779737fc21d",
      "value": " 1479/1479 [17:51&lt;00:00,  1.69it/s]"
     }
    },
    "8ff6f8e7583545009a114779737fc21d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90211e78352f4c8d937977ff69356f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bae1963aee74b758b14fa80d6c73bd8",
      "max": 11828,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad578853ba7a436cb60d7afbfac33fe2",
      "value": 1769
     }
    },
    "92394a999ba648489187f8640a3103f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80f8797be6114b2ca926328de770a154",
      "placeholder": "​",
      "style": "IPY_MODEL_897fcb0d776c488a921a20dafa493646",
      "value": " 15%"
     }
    },
    "a5bb6be7b50840ddb78e799b12cdae5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57239a592386457683607013e2b5889b",
      "max": 1479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04f9af659c9a4c6a8353de757c860f31",
      "value": 1479
     }
    },
    "aafd5fd722704233b9e4f2d14513dcef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad578853ba7a436cb60d7afbfac33fe2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aef9d9e4615841c1a39cf6c6691f7d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92394a999ba648489187f8640a3103f7",
       "IPY_MODEL_90211e78352f4c8d937977ff69356f9a",
       "IPY_MODEL_09f0b29108a548368f5938db42157959"
      ],
      "layout": "IPY_MODEL_12001d20aae1463497f64dcccf5ec326"
     }
    },
    "ba36f81d841e4f379d118a25d34a676e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0ae577f2dd24c7c8a6a386bc37c7fc8",
      "placeholder": "​",
      "style": "IPY_MODEL_aafd5fd722704233b9e4f2d14513dcef",
      "value": "100%"
     }
    },
    "c745a589d5e5447d88b88d189fc4589a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e50df2a91f5a489ea4cfa6c8e0e9771f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0ae577f2dd24c7c8a6a386bc37c7fc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
